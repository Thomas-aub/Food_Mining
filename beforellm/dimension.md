## Pourquoi faire de la r√©duction de dimension

Dans notre cas, on part d‚Äôune **matrice ‚Äúrecettes √ó ingr√©dients‚Äù**, de tr√®s grande taille :

* quelques milliers de recettes,
* plusieurs centaines voire milliers d‚Äôingr√©dients.

üëâ Chaque ingr√©dient est une **dimension**.
On a donc un espace **hautement dimensionnel et tr√®s creux** (sparse).

**Cons√©quences :**

* Les distances (cosine, Jaccard, euclidienne‚Ä¶) deviennent **peu discriminantes** ‚Äî beaucoup de points sont ‚Äúpresque √† √©gale distance‚Äù.
* Les algorithmes de clustering (DBSCAN, KMeans‚Ä¶) perdent en performance.
* La visualisation devient impossible directement (plus de 2‚Äì3 dimensions).

La **r√©duction de dimension** permet donc de :

* **Projeter** ces donn√©es dans un espace plus petit (ex : 2D ou 3D) pour les visualiser ;
* **D√©nser** l√©g√®rement les donn√©es ‚Üí distances plus significatives ;
* **Acc√©l√©rer** les algorithmes de clustering et d‚Äôanalyse ;
* **D√©bruiter** la matrice initiale et faire ressortir les relations latentes entre ingr√©dients.

---

## 2. M√©thodes possibles 



| Objectif                     | M√©thode                                     | Adapt√©e √†                  | Avantages                                                | Inconv√©nients                    |
| ---------------------------- | ------------------------------------------- | -------------------------- | -------------------------------------------------------- | -------------------------------- |
| Pr√©parer clustering rapide   | **TruncatedSVD** (PCA pour matrices sparse) | TF-IDF ou binaire sparse   | Rapide, lin√©aire, compatible sklearn                     | lin√©aire uniquement              |
| Visualisation claire         | **t-SNE** ou **UMAP**                       | donn√©es denses ou r√©duites | capte non-lin√©arit√©s, tr√®s lisible                       | plus lent, d√©pend des param√®tres |
| D√©tection de structure dense | **UMAP + HDBSCAN**                          | donn√©es sparse             | tr√®s bon combo pour data mining exploratoire             | plus complexe √† param√©trer       |
| Extraction de th√©matiques    | **NMF** (Non-negative Matrix Factorization) | TF-IDF                     | interpr√©table (chaque composante = groupe d‚Äôingr√©dients) | n√©cessite tuning du rang         |

---

## Enjeux et limites

### üîç Enjeux

* **Compr√©hension** : visualiser des patterns, tendances, familles de recettes.
* **Communication** : pr√©senter les r√©sultats √† un public non technique.
* **Feature engineering** : utiliser les composantes r√©duites comme ‚Äúfacteurs latents‚Äù dans un mod√®le de recommandation (ex : ‚Äúrecettes proches‚Äù).
* **Aide au choix du clustering** : si les points se s√©parent mal, il faut revoir la m√©trique ou la repr√©sentation.

### ‚ö†Ô∏è Limites

* t-SNE / UMAP **ne pr√©servent pas les distances globales** (juste les voisinages).
  ‚Üí Bonne visualisation, mais pas une ‚Äúpreuve‚Äù que les clusters sont s√©par√©s num√©riquement.
* La **stabilit√©** des projections d√©pend du random_state et des hyperparam√®tres.
* Attention √† la **taille m√©moire** : t-SNE et UMAP ne scalent pas bien sur 100k+ points.
